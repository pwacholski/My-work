{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Implementation of multi-layer neural network using NumPy**"},{"metadata":{},"cell_type":"markdown","source":"**Import libraries**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, date, time","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Network parameters**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"no_of_samples = 10000\nno_of_columns = 160\nno_of_classes = 2\ninitial_weights_range = 0.075\nlearning_rate = 0.05\nbatch_size = 20\ntrain_index = int(0.9 * no_of_samples / batch_size) * batch_size\ntest_end_index = int(no_of_samples / batch_size) * batch_size\nneurons_cnt_list =   list((128, 64, no_of_classes)) # format of the list: layer_1, layer_2,.....,layer_n, no_of_classes. For example (512, 128, 128, no_of_classes)","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Crate dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\nnormal_sample = np.random.normal(loc=0, scale=1,size= (no_of_samples,no_of_columns))\n\nnormal_sample_class = np.where( np.logical_or( (0.6 * normal_sample[:,4].reshape(no_of_samples,1) + 1.2 * normal_sample[:,2].reshape(no_of_samples,1)) > 1.5,\n                                             (0.5 * normal_sample[:,0].reshape(no_of_samples,1) + normal_sample[:,1].reshape(no_of_samples,1)) < -0.33), np.ones((no_of_samples,1 ), dtype = 'int64'), np.zeros((no_of_samples,1 ), dtype = 'int64')  )\n\nnormal_sample_class_dumm = pd.get_dummies(normal_sample_class[:,0]).values","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model definition**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_layers_dim_list(tmp_neuron_list, elem_to_add, acc, tmp_no_of_classes):\n        if len(tmp_neuron_list) > 0:\n                acc.append((elem_to_add, tmp_neuron_list[0]))\n                return create_layers_dim_list(tmp_neuron_list[1:], tmp_neuron_list[0], acc, tmp_no_of_classes)\n        else:\n                return acc\n \n    \ndef activation_function(x):\n        return 1/(1+np.exp(-x))\n\ndef activation_derivative(x):\n           return x*(1-x)\n\n\nclass NeuralNetworkModel:\n    \n\n\n    def __init__(self,  tmp_neruons_list, tmp_no_of_cols, tmp_no_of_classes, tmp_initial_weights_range, seed):\n        np.random.seed(seed)\n        self.biases_list = [ (2 * tmp_initial_weights_range * np.random.rand(  1 ,  curr_elem )) - tmp_initial_weights_range for curr_elem in  tmp_neruons_list]\n        self.dim_list = create_layers_dim_list( tmp_neruons_list, tmp_no_of_cols, [], tmp_no_of_classes)\n        self.weights_list = [ (2 * tmp_initial_weights_range * np.random.rand(curr_elem[0], curr_elem[1])) - tmp_initial_weights_range for curr_elem in self.dim_list]\n        self.no_of_layers = len(tmp_neruons_list)\n\n    def propagate_activations(self, tmp_input_data, tmp_layer_no, tmp_total_layer_cnt, tmp_act_input_list, tmp_act_list):\n        if tmp_layer_no < tmp_total_layer_cnt:\n            tmp_act_input_list.append(np.dot(tmp_input_data, self.weights_list[tmp_layer_no]) + self.biases_list[tmp_layer_no])\n            tmp_act_list.append(activation_function (tmp_act_input_list[tmp_layer_no])) \n            return self.propagate_activations(tmp_act_list[tmp_layer_no], tmp_layer_no + 1, tmp_total_layer_cnt, tmp_act_input_list, tmp_act_list )\n        else: \n            return (tmp_act_input_list, tmp_act_list)\n\n    def error_backpropagation(self, tmp_delta_l_up, tmp_current_layer, tmp_learning_rate, tmp_input_data, tmp_act_list):\n        if tmp_current_layer > 0:\n            tmp_delta = np.dot(  tmp_delta_l_up,  self.weights_list[tmp_current_layer].T ) * activation_derivative(tmp_act_list[tmp_current_layer-1] )\n            self.weights_list[tmp_current_layer] -= ((tmp_learning_rate) * np.dot(tmp_act_list[tmp_current_layer-1].T,  tmp_delta_l_up) )\n            self.biases_list[tmp_current_layer] -= ((tmp_learning_rate) * np.sum(tmp_delta_l_up, axis = 0))\n            self.error_backpropagation(tmp_delta, tmp_current_layer - 1, tmp_learning_rate, tmp_input_data, tmp_act_list)\n        else:\n            self.weights_list[tmp_current_layer] -= ((tmp_learning_rate) * np.dot(tmp_input_data.T,  tmp_delta_l_up) )\n            self.biases_list[tmp_current_layer] -= ((tmp_learning_rate) * np.sum(tmp_delta_l_up, axis = 0))\n\n\n    def process_batch(self, tmp_batch_indicies, tmp_train2, tmp_class2):\n        tmp_train = tmp_train2[tmp_batch_indicies]\n        tmp_class = tmp_class2[tmp_batch_indicies]\n        activations_list = self.propagate_activations( tmp_train, 0, self.no_of_layers, [], [])[1]\n        error_last_layer =   activations_list[self.no_of_layers -1] - tmp_class\n        delta_last_layer = error_last_layer * activation_derivative(activations_list[self.no_of_layers -1] )\n        self.error_backpropagation (delta_last_layer, self.no_of_layers -1, (learning_rate / tmp_train.shape[0]), tmp_train, activations_list)\n\n\n    def process_one_epoch(self,  tmp_train_index, tmp_tr, tmp_cls ):\n        batch_cntrl_matrix = np.arange(tmp_train_index).reshape(-1,batch_size)\n        np.apply_along_axis( self.process_batch, 1 , batch_cntrl_matrix, tmp_tr, tmp_cls )\n    \n    \n    def fit(self, tmp_train_data, tmp_class_data, tmp_train_index1, no_of_epochs):\n        for _ in range(no_of_epochs):\n            self.process_one_epoch( tmp_train_index1, tmp_train_data, tmp_class_data)\n        \n    \n    def predict(self, tmp_test_data):\n        return np.argmax((self.propagate_activations( tmp_test_data, 0, self.no_of_layers, [], [])[1])[self.no_of_layers -1] ,axis=1)\n","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create an instance of the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmpModel = NeuralNetworkModel(neurons_cnt_list, no_of_columns, no_of_classes, initial_weights_range, seed = 1)","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tmpModel.fit(normal_sample, normal_sample_class_dumm , train_index, no_of_epochs = 150)","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predict**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(normal_sample_class[train_index:test_end_index,0], tmpModel.predict( normal_sample[train_index:test_end_index ,:]))","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"col_0    0    1\nrow_0          \n0      473   30\n1       84  413","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>col_0</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>row_0</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>473</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>84</td>\n      <td>413</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}